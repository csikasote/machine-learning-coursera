{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING NECESSARY LIBRARIES\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio #For loading OCTAVE .mat files #REF: https://docs.scipy.org/doc/scipy/reference/tutorial/io.html\n",
    "import matplotlib.cm as cm #Used to display images in a specific colormap\n",
    "import random #To pick random images to display\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15.0,10.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "#np.random.seed(1)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK MODEL SETTINGS FOR THE EXERCISE\n",
    "input_layer_size  = 1;  # 20x20 Input Images of Digits\n",
    "hidden_layer_size = 4;   # 25 hidden units\n",
    "num_labels = 1;          # 10 labels, from 1 to 10 (note that we have mapped \"0\" to label 10)\n",
    "lambda_val = 0.01;       # Lambda value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (3, 1)\n",
      "y_train shape: (3, 1)\n",
      "nn_weights shape: (13, 1)\n"
     ]
    }
   ],
   "source": [
    "nn_weights =np.array([3.1,1.6,1.5,-2.9,-1.3,-0.8,-0.7,1.3,5.4,-1.7,-1.1,-0.9,1.6]).reshape(-1,1)\n",
    "X_train = np.array([1,2,3]).reshape(-1,1)\n",
    "y_train = np.array([1,4,9]).reshape(-1,1)\n",
    "\n",
    "#RESULTS CHECK\n",
    "\n",
    "print('X_train shape:',X_train.shape)\n",
    "print('y_train shape:',y_train.shape)\n",
    "print('nn_weights shape:',nn_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZING PARAMETERS\n",
    "def initialize_parameters(nn_weights, input_layer_size, hidden_layer_size, num_labels):\n",
    "    # -------------------------------------------------------------\n",
    "    # Reshaping W1 and W2\n",
    "    # -------------------------------------------------------------\n",
    "    W1 = np.reshape(nn_weights[0:hidden_layer_size * (input_layer_size + 1)], (hidden_layer_size, (input_layer_size + 1)));\n",
    "    W2 = np.reshape(nn_weights[(hidden_layer_size * (input_layer_size + 1)):,], (num_labels, (hidden_layer_size + 1)));\n",
    "    parameters = {\"W1\":W1,\n",
    "                  \"W2\":W2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 shape: (4, 2)\n",
      "W2 shape: (1, 5)\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(nn_weights, input_layer_size, hidden_layer_size, num_labels);\n",
    "\n",
    "#RESULTS CHECK\n",
    "print('W1 shape:',parameters['W1'].shape)\n",
    "print('W2 shape:',parameters['W2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictLinear(x,theta):\n",
    "    return np.dot(x,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORWARD PROPAGATION FUNCTION\n",
    "\n",
    "def forward_propagation(X,parameters):\n",
    "    \n",
    "    # -------------------------------------------------------------\n",
    "    # Retrieving parameters\n",
    "    # -------------------------------------------------------------\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    m = X.shape[0];\n",
    "        \n",
    "    # -------------------------------------------------------------\n",
    "    # The Forward Propagation Implementation\n",
    "    # -------------------------------------------------------------\n",
    "    A1   = np.insert(X,0,1,axis=1);     # (3,2)\n",
    "    Z2   = np.dot(A1,W1.T);             # (3,2).(2,4) = (3,4)\n",
    "    tanh = np.tanh(Z2);                 # (3,4)\n",
    "    A2   = np.insert(tanh,0,1,axis=1);  # (3,5)\n",
    "    Z3   = predictLinear(A2,W2.T);      # (3,5).(5,1) = (3,1)\n",
    " \n",
    "    # -------------------------------------------------------------\n",
    "    # SAVING THE ACTIVATIONS TO DICTIONARY\n",
    "    # -------------------------------------------------------------\n",
    "    cache = {\"A1\":A1, \n",
    "             \"Z2\":Z2,\n",
    "             \"A2\":A2,\n",
    "             \"Z3\":Z3}\n",
    "\n",
    "    return Z3, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COST FUNCTION TO COMPUTE THE MEAN SQUARED ERROR\n",
    "\n",
    "def nnCostFunctionLinear(Z3, y, parameters,lambda_val):\n",
    "    #theta = parameters[\"W2\"].T\n",
    "    m = Z3.shape[0]; \n",
    "    error = Z3-y;\n",
    "    error_sqrd = np.square(error);\n",
    "    sum_error_sqrd = np.sum(error_sqrd) \n",
    "    unreg_cost =(1/(2*m)) * sum_error_sqrd;\n",
    "    #temp = np.ones((theta.shape[0],1))\n",
    "    #temp[0,0] = 0\n",
    "    #reg_term = (lambda_val/(2*m)) * np.sum(np.square(np.multiply(temp, theta)))#np.sum(np.dot(theta[1:].T,theta[1:])); #lambda_t/2/m*np.sum(np.multiply(mask_array, theta)**2)\n",
    "    cost = unreg_cost #+ reg_term\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cost at parameters (loaded): 7.090178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(nn_weights, input_layer_size, hidden_layer_size, num_labels);\n",
    "Z3, cache = forward_propagation(X_train,parameters)\n",
    "cost = nnCostFunctionLinear(Z3, y_train, parameters, lambda_val)\n",
    "print('\\nCost at parameters (loaded): %.6f\\n' % (cost));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randInitializeWeights(L_in, L_out):\n",
    "    W = np.zeros((L_out, 1 + L_in));\n",
    "    epsilon_init = np.sqrt(6)/np.sqrt(L_in + L_out); #0.12;\n",
    "    W = np.random.randn(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init;\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing Neural Network Parameters ...\n",
      "\n",
      "Shape of initial_W1 is:  (4, 2)\n",
      "Shape of initial_W2 is:  (1, 5)\n",
      "Shape of nn_params is:  (13, 1)\n"
     ]
    }
   ],
   "source": [
    "# Initializing NN Weights\n",
    "\n",
    "print('\\nInitializing Neural Network Parameters ...\\n')\n",
    "initial_W1 = randInitializeWeights(input_layer_size, hidden_layer_size);\n",
    "initial_W2 = randInitializeWeights(hidden_layer_size, num_labels);\n",
    "\n",
    "# Flattening the weights\n",
    "initial_W1_flat = initial_W1.flatten()\n",
    "initial_W2_flat = initial_W2.flatten()\n",
    "\n",
    "# Unroll parameters\n",
    "initial_nn_params = np.concatenate((initial_W1_flat,initial_W2_flat),axis=0).reshape(-1,1)\n",
    "\n",
    "#RESULTS CHECK\n",
    "print('Shape of initial_W1 is: ', initial_W1.shape)\n",
    "print('Shape of initial_W2 is: ', initial_W2.shape)\n",
    "print('Shape of nn_params is: ', initial_nn_params.shape)\n",
    "\n",
    "#print('Initial_W1 is: ', initial_W1)\n",
    "#print('Initial_W2 is: ', initial_W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward  propagation function\n",
    "\n",
    "def backward_propagation(parameters, cache, x, y, lambda_val):\n",
    "    # -------------------------------------------------------------\n",
    "    # Retrieving parameters\n",
    "    # -------------------------------------------------------------\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # Retrieving cache parameters\n",
    "    # -------------------------------------------------------------\n",
    "    A1 = cache[\"A1\"]     # (3,2)\n",
    "    Z2 = cache[\"Z2\"]     # (3,4)\n",
    "    A2 = cache[\"A2\"]     # (3,5)\n",
    "    Z3 = cache[\"Z3\"]     # (3,1)\n",
    "\n",
    "\n",
    "    m = x.shape[0];\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # Backpropagation algorithm\n",
    "    # -------------------------------------------------------------\n",
    "    \n",
    "    delta2 = (1/m)*np.dot((Z3 - y).T,A2)            \n",
    "    u = np.tanh(Z2)\n",
    "    d_tanh = 1 - np.power(u,2)\n",
    "    d2a = np.dot((Z3 - y),W2[:,1:])\n",
    "    d2 = np.multiply(d2a, d_tanh)\n",
    "    delta1 = (1/m) * np.dot(d2.T,A1)\n",
    "    temp1=W1; \n",
    "    temp2=W2;\n",
    "    temp1[:,0]=0;\n",
    "    temp2[:,0]=0;\n",
    "    dW1 = delta1 + ((lambda_val/m) * temp1)\n",
    "    dW2 = delta2 + ((lambda_val/m) * temp2)\n",
    "    \n",
    "    grads = {\"dW1\":dW1,\n",
    "             \"dW2\":dW2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters,grads, learning_rate):\n",
    "    # -------------------------------------------------------------\n",
    "    # Retrieving parameters\n",
    "    # -------------------------------------------------------------\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    \n",
    "    # -------------------------------------------------------------\n",
    "    # Retrieving gradients\n",
    "    # -------------------------------------------------------------\n",
    "    dW1 = grads[\"dW1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    # -------------------------------------------------------------\n",
    "    # Update parameters\n",
    "    # -------------------------------------------------------------\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    \n",
    "    \n",
    "    parameters = {\"W1\":W1,\n",
    "                  \"W2\":W2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X,Y, initial_nn_params, input_layer_size, hidden_layer_size, num_labels, lambda_val, print_cost=False):\n",
    "    \n",
    "    parameters = initialize_parameters(initial_nn_params, input_layer_size, hidden_layer_size, num_labels)\n",
    "        \n",
    "    # -------------------------------------------------------------\n",
    "    # Retrieving parameters\n",
    "    # -------------------------------------------------------------\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    for i in range(10000):\n",
    "        Z3, cache = forward_propagation(X,parameters)\n",
    "        cost = nnCostFunctionLinear(Z3, Y, parameters, lambda_val)\n",
    "        grads = backward_propagation(parameters, cache, X, Y, lambda_val)\n",
    "        parameters = update_parameters(parameters,grads, learning_rate= 0.006)\n",
    "        if print_cost and i % 500 ==0:\n",
    "            print(\"Iteration %i: Cost:%f\"%(i,cost))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Cost:18.258947\n",
      "Iteration 500: Cost:5.409391\n",
      "Iteration 1000: Cost:5.372220\n",
      "Iteration 1500: Cost:4.837369\n",
      "Iteration 2000: Cost:3.519476\n",
      "Iteration 2500: Cost:2.971702\n",
      "Iteration 3000: Cost:2.517666\n",
      "Iteration 3500: Cost:2.142860\n",
      "Iteration 4000: Cost:1.834213\n",
      "Iteration 4500: Cost:1.580312\n",
      "Iteration 5000: Cost:1.371420\n",
      "Iteration 5500: Cost:1.199382\n",
      "Iteration 6000: Cost:1.057441\n",
      "Iteration 6500: Cost:0.940051\n",
      "Iteration 7000: Cost:0.842680\n",
      "Iteration 7500: Cost:0.761641\n",
      "Iteration 8000: Cost:1.017819\n",
      "Iteration 8500: Cost:0.933171\n",
      "Iteration 9000: Cost:0.946310\n",
      "Iteration 9500: Cost:0.956946\n"
     ]
    }
   ],
   "source": [
    "parameters = nn_model(X_train,y_train, initial_nn_params, input_layer_size, hidden_layer_size, num_labels, lambda_val, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
