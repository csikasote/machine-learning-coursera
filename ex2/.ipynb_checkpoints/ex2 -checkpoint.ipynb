{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Exercise 2 (Part A): Logistic regression w/t Regularization#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the second programming exercise! In this exercise, you will implement logistic regression and apply it to two\n",
    "different datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly, Logistic regression (or Logit regression) model is another class of regression models that is used to `estimate` the probability that a given instance of a dataset belongs to a particular class. You may think of a classic example of spam and non-spam emails. \n",
    "The idea is that if the estimated probality is greater than `50%`, then the model predicts that the instance belongs to a positve class, say 'A', else it predicts that it belongs to negative class, B. This definition makes the Logistic regression model a `binary classifier`.\n",
    "\n",
    "In order to estimate probabilities, the Logistic regression model computes a weighted sum of the input features (plus a bias term). However, unlike the Linear regression model which outputs the result directly, it computes the logistic of this result. \n",
    "\n",
    "This is given by the following equation: ${\\hat P} = {h_\\theta{(x)}} = {\\sigma {(\\theta^T.x)}}$, where the logistic - noted $\\sigma(.)$ is a sigmoid function that outputs a number between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instruction:**\n",
    "- To run code, click on a corresponding cell and press `Shift+Enter` keys simultaneously or Click `Cell -> Run` Cells.\n",
    "\n",
    "**Objective:**\n",
    "- To implement Logistic regression to classify data points into discrete outcomes.\n",
    "\n",
    "**You will learn how to:**\n",
    "- implement Logistic regression \n",
    "    - objective/cost function\n",
    "    - scipy for optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "First lets run the cell below to import all the packages that you will need for this exercise.\n",
    "- [NumPy](http://www.numpy.org) is the fundamental package for scientific computing with Python\n",
    "- [Matplotlib](http://matplotlib.org) is a common library to plot graphs in python.\n",
    "- [os](https://docs.python.org/3/library/os.html#) is a Python module that provides a portable way of using operating system dependent functionality\n",
    "- [Scipy](https://docs.scipy.org/doc/scipy-0.17.0/reference/optimize.html) a python library here used for optimization functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python â‰¥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Common imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# For optimization\n",
    "from scipy import optimize as opt\n",
    "\n",
    "# To make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# IMPORTING NECESSARY LIBRARIES\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Overview of the problem ##\n",
    "\n",
    "Suppose that you are the administrator of a university department and you want to determine each applicant's chance of admission based on their results on two exams. You have historical data from previous applicants that you can use as a training set for logistic regression. For each training example, you have the applicant's scores on two exams and the admissions decision.\n",
    "\n",
    "Your task is to build a classification model that estimates an applicant's probability of admission based the scores from those two exams.\n",
    "\n",
    "Therefore, you will build a logistic regression model to predict whether a student gets admitted into a university."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Loading dataset and visualizing the training data##\n",
    "\n",
    "Before we embark on the task at hand, it is useful to understand the data distribution by visualizing it. For this dataset, we will use a scatter plot to visualize the data, since it has only two properties to plot (profit and population). (Many other problems that you will encounter in real life are multi-dimensional and can't be plotted on a 2-d plot.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Loading dataset analysis ###\n",
    "\n",
    "The code in the cell below loads the data using the pandas's `read_csv()` function. The dataset is loaded from the data file into the variables X and y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd() + '\\data\\ex2data1.txt'\n",
    "df = pd.read_csv(path, header=None, names=['Exam 1', 'Exam 2', 'Admitted'])\n",
    "data=df.values\n",
    "X = data[:,[0,1]]\n",
    "Y = data[:,[2]].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Plot dataset points ###\n",
    "\n",
    "We now define a function to plot the dataset points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZING DATA DISTRIBUTION\n",
    "pos_df = df[df['Admitted'].isin([1])] # df with records having only ones (1) in the label column 'Admitted' - POSITIVE\n",
    "neg_df = df[df['Admitted'].isin([0])] # df with records having zeros (0) in the label column 'Admitted' - NEGATIVE\n",
    "Ex1_pos = pos_df['Exam 1'] \n",
    "Ex2_pos = pos_df['Exam 2']\n",
    "Ex1_neg = neg_df['Exam 1']\n",
    "Ex2_neg = neg_df['Exam 2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINING A DATA PLOT FUNCTION\n",
    "def plotData(x1,x2,y1,y2): \n",
    "    plt.scatter(x1,y1, s=50, c='b', marker='o', label='Admitted')  \n",
    "    plt.scatter(x2,y2, s=50, c='r', marker='x', label='Not Admitted')  \n",
    "    plt.xlabel('Exam 1 Score')  \n",
    "    plt.ylabel('Exam 2 Score')\n",
    "    plt.title('Scatter plot of Trainig data')\n",
    "    plt.axis([20,110,20,110])\n",
    "    plt.grid(True)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotData(Ex1_pos,Ex1_neg,Ex2_pos,Ex2_neg) # CALLING THE FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Implementation of logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 -  Initial settings ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZATIONS\n",
    "(m,n) = X.shape\n",
    "num_iters = 400\n",
    "theta = np.zeros((n+1, 1)); # initialize fitting parameters to zeros\n",
    "X = np.c_[np.ones((m, 1)), X]; # Add a column of ones to x for x0 = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Sigmoid function ###\n",
    "\n",
    "Recall that the logistic regression hypothesis is defined as:\n",
    "\n",
    "$${\\hat p} = {h_\\theta{(x)}} = {\\sigma{(\\theta^Tx)}} \\tag{1}$$\n",
    "\n",
    "where function **$\\sigma(.)$** is the `sigmoid function`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The sigmoid function  also refered to as `logistic function` or a  `logit` is defined as:\n",
    " $${\\sigma{(z)}} = \\frac{1}{ 1 + {e^{-z}}} \\tag{2}$$\n",
    " \n",
    " with respect to Logistic regression: $${z} = {\\theta^T.\\,{x}} \\tag{3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per the requirement in this exercise, we implement this function as `sigmoid()` in the cell below.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINITION OF THE SIGMOID/LOGISTIC FUNCTION\n",
    "def sigmoid(z):\n",
    "    return (1/(1+ np.exp(-z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now thet we have implemented the sigmoid fucntion, we can visualize the sigmoid function by `running` the cell below to observe the behaviour of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AxesSubplot' object has no attribute 'ylabel'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-7836b64f9cec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# CALLING THE SIGMOID FUNCTION\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0msigmoidPlot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-7836b64f9cec>\u001b[0m in \u001b[0;36msigmoidPlot\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'X'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"$\\sigma(.)$\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m17\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"$z$\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m17\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xlim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'AxesSubplot' object has no attribute 'ylabel'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CHECKING SIGMOID FUNCTION BY PLOTTING\n",
    "x_vals = np.linspace(-10, 10, 50) # DEFINING THE X AXIS \n",
    "def sigmoidPlot(x):\n",
    "    y = sigmoid(x)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x,y, 'r', label='Sigmoid/Logistic Function')\n",
    "    ax.plot(0,0.5,'X')\n",
    "    ax.grid(True)\n",
    "    ax.ylabel(r\"$\\sigma(.)$\", fontsize=17)\n",
    "    ax.xlabel(r\"$z$\", fontsize=17)\n",
    "    ax.set_xlim([-10,10])\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.legend()\n",
    "\n",
    "# CALLING THE SIGMOID FUNCTION\n",
    "sigmoidPlot(x_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic function g(z), shown above, maps any real number to the (0, 1) interval, making it useful for transforming an arbitrary-valued function into a function better suited for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement the ${z} = {\\theta^T.{x}}$ as in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTION/HYPOTHESIS FUNCTION\n",
    "def z(x, theta):\n",
    "    return np.dot(x,theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2 - Objective function**\n",
    "\n",
    "We cannot use the same cost function that we use for linear regression because the Logistic Function will cause the output to be wavy, causing many local optima. In other words, it will not be a convex function.\n",
    "\n",
    "Therefore, with Logistic regression, the objective function is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ {J(\\theta)} = \\frac{1}{m} \\sum_{i=1}^m Cost\\,(h_\\theta(x^{(i)})\\,, y^{(i)}) \\tag{4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- if **y = 1:**$\\;{Cost\\,(h_\\theta \\,{(x)} , y)} = {-log\\,{(h_\\theta {(x)})}}$\n",
    "\n",
    "- if **y = 0:**$\\;{Cost\\,(h_\\theta \\,{(x)} , y)} = {-log\\,{(1 - h_\\theta {(x)})}}$\n",
    "\n",
    "\n",
    "We can compress our cost function's two conditional cases into one case:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${Cost\\,(h_\\theta \\,{(x)}\\,, y)} = {-y\\,log\\,{(h_\\theta {(x)})} -(1-y)\\,log\\,{(1 - h_\\theta {(x)})}}\\tag{5}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- when y = 1, then the second term ${(1-y)\\,log\\,{(1 - h_\\theta {(x)})}}$ will be zero and will not affect the result.\n",
    "\n",
    "\n",
    "\n",
    "- when y = 0, then the first term ${-y\\,log\\,{(h_\\theta {(x)})}}$ will be zero and will not affect the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then fully write out our entire cost function as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${J{(\\theta)}} = {-\\frac{1}{m}}{\\sum_{i=1}^m}{[{y^{(i)}\\,log\\,{(h_\\theta (x^{(i)})})}  + (1-y)^{(i)}\\,log\\,{(1 - h_\\theta (x^ {(i)})})]}\\tag{6}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and its `vectorized form` is: \n",
    "\n",
    "$${J{(\\theta)}} = \\frac{1}{m}.{({-y^T\\,log{(h)} -(1-y)^T\\,log{(1 - h)}})}\\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINITION OF THE LOGISTIC COST FUNCTION\n",
    "def costFunction(theta, x, y):\n",
    "    m = len(x)\n",
    "    h = z(x, theta)\n",
    "    pred = sigmoid(h)\n",
    "    cost = (-1/m) * (np.dot(y.T,np.log(pred)) + np.dot((1-y).T,np.log(1-pred)))\n",
    "    return cost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us evaluate our implementation of the objective function on initial theta. `Run` the cell below to check results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTING INITIAL COST WITH INITIAL THETA\n",
    "cost = costFunction(theta, X, Y) # Calling the logistic cost function\n",
    "print('\\nInitial Cost: %f' % (cost[0,0]) + ', Theta: ' + str(theta[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Logistic regression gradient ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the general form of gradient descent is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat {\n",
    "\n",
    "$ \\theta_j = \\theta_j - \\alpha \\frac {\\partial}{{\\partial} {\\theta_j}}{J{(\\theta)}}$\n",
    "    \n",
    "   }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where the derivative of the cost function is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ {\\frac {\\partial J(\\theta)}{\\partial \\theta_j}} = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)})-y^{(i)})\\,x_j^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **gradient descent algorithm** will repeat until convergence {\n",
    "$$\\theta_{j} = \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)})-y^{(i)}).x_j^{(i)} \\tag{8}$$\n",
    "} for **j = 0...n**, where $x_0^{(i)}$ = **1**, **m** is the size of the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `vectorized implementation` is: $$\\theta_{j} = \\theta_ {j} - \\frac {\\alpha}{m}{X^T(\\sigma{(X \\theta) - \\vec y})} \\tag{9}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where ${X \\theta}$ is the `vectorized form` of ${\\theta^{T}x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 - Learning parameters using `fmin()` ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous exercise, we found the optimal parameters of a linear regression model by implementing gradent descent. We wrote a cost function and calculated its gradient, then took a gradient descent step accordingly.\n",
    "\n",
    "However, besides gradient descent, there are other advanced optimization technoques that can be employed for the purpose of optimizing the objective function.\n",
    "\n",
    "In this exercise, we shall use the `fmin()` optimization technique to optimize the objective function. You can read more about it [here](https://docs.scipy.org/doc/scipy-0.17.0/reference/generated/scipy.optimize.fmin.html#scipy.optimize.fmin).  Therefore, we create a function `optimizeCostFunction()` that is going to take in parameters: objective function,`func`; theta, `initial_theta`; input features, `x`; target class, `y` and the number of iterations, `iters`. \n",
    "\n",
    "The function, if terminates successfully, outputs the $\\theta$ values ogf the `minimum point` and the `cost (error)`\n",
    "at the minimum point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `optimizeCostFunction()` below implements the calling of the `fmin()` function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizeCostFunction(func, theta,x,y,iters):\n",
    "    result = opt.fmin(func=func, x0=theta, args=(x,y),maxiter=iters, full_output=1)\n",
    "    return result[0], result[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate or implementation of our optimization function to ensure its terminating successfully. `Run` the cell below .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta,cost = optimizeCostFunction(costFunction,theta,X,Y, num_iters)\n",
    "print('\\nResults are as follows:\\n\\nTheta found by fmin: '\\\n",
    "      + str(theta) + '\\nCost at theta found by fmin: %.5f' % (cost));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 - Evaluating logistic regression ###\n",
    "\n",
    "After learning the parameters, you can use the model to predict whether a particular student will be admitted. For a student with an Exam 1 score of 45 and an Exam 2 score of 85, we should expect to see an admission probability of `0.776`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we need to define a function that computes an estimated probability of the given instance given by the equation: ${\\hat p} = {\\sigma{(h_\\theta {(x)})}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta, x):  \n",
    "    probability = sigmoid(np.dot(x,theta))\n",
    "    return [1 if x >= 0.5 else 0 for x in probability]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the model, we can assess its performance against the true values in the target class. This is what we are referring to as training accuracy. The sum of correct/true predictions against the sum of all predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTING ACCURACY\n",
    "predictions = predict(theta, X)  \n",
    "correct = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, Y)]  \n",
    "accuracy = (sum(map(int, correct)) % len(correct))  \n",
    "print('Training Accuracy(UNPARTITIONED DATA) = {0}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ... from our computations, we can safely say, our model predicted `89%` of the instances in the dataset as true target classes in **y**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(theta, x):  \n",
    "    probability = sigmoid(np.dot(x,theta))\n",
    "    return [\"ADMIT\" if x >= 0.5 else \"DENY\" for x in probability]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_decision(x,theta):\n",
    "    prob = sigmoid(np.dot(x,theta))\n",
    "    pred = predict(theta, x)\n",
    "    clf = classify(theta, x)\n",
    "    print('Probability of admission: %2f' % (prob[0]) \\\n",
    "          + ' \\nPrediciton: ' + str(pred[0]) \\\n",
    "          + ' \\nModel Decision: ' + str(clf[0]) \\\n",
    "          + ' STUDENT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `logistic_decision()` function above computes the `estimated probability`, `prediction (1 or 0)` and `a descrete decision (ADMIT or DENY)` based on the marks of a student in Exam 1 and Exam 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_x = [[1, 45, 85]] #TEST EXAMPLE\n",
    "logistic_decision(student_x,theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 - Training data with decision boundary ###\n",
    "\n",
    "At this particular point of our exercise, we have $\\theta$ values at the minimum point. This implies we can plot a `decision boundary line` that separates the two classes, ADMITTED or DENIED students in our training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute the boundary line, we need to remember that for logistic regression, ${h} = {\\sigma {(X . \\theta)}}$.. This describes the relationship between X, theta, and h.\n",
    "\n",
    "Now, by definition, the decision boundary is the `locus of points` where h = 0.5, or equivalently (X * theta) = 0, since the ${\\sigma {(0)}}$ is 0.5.\n",
    "\n",
    "Therefore, we can write out the equation for the case where we have two features and a bias unit, and we write **X** as $[ {x_0,x_1, x_2}]$ and theta as $[\\theta_0, \\theta_1, \\theta_2]$\n",
    "\n",
    "${0} = {\\theta_0x_0 + \\theta_1 x_1 + \\theta_2 x_2} \\tag{10}$\n",
    "\n",
    "${x_0}$ is the bias unit, it is hard-coded to 1. Now we solve for ${x_2}$..\n",
    "\n",
    "$${x_2} = {\\frac {-(\\theta_0 + \\theta_1 x_1)}{\\theta_2}} \\tag{11}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotDecisionBoundary(x1,x2,y1,y2,theta):\n",
    "    theta0 = theta[0]; \n",
    "    theta1 = theta[1]; \n",
    "    theta2 = theta[2];\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.scatter(x1,y1, s=50, c='b', marker='o', label='Admitted')  \n",
    "    plt.scatter(x2,y2, s=50, c='r', marker='x', label='Not Admitted')\n",
    "    x_vals = np.array(ax.get_xlim())\n",
    "    y_vals = -1 * np.divide(((np.multiply(theta1,x_vals)) + theta0),theta2)\n",
    "    plt.plot(x_vals, y_vals, '--', c=\"red\", label='Decision Boundary')\n",
    "    titlestr = 'Decision Boundary Function: y = %.2f + %.2fX1 + %.2fX2' % (theta0, theta1, theta2)\n",
    "    plt.title(titlestr) \n",
    "    plt.axis([20,110,20,110])\n",
    "    plt.xlabel('Exam 1 Score')  \n",
    "    plt.ylabel('Exam 2 Score')\n",
    "    plt.grid(True)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotDecisionBoundary(Ex1_pos,Ex1_neg,Ex2_pos,Ex2_neg, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Exercise 2 (Part B): Regularized logistic regression#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand `regularization`, we need to know two phenomenons associated with machine learning algorithms: underfitting (high bias) and overfitting (high variance).\n",
    "\n",
    "`Underfitting`, or `high bias`, is when the form of our hypothesis function ${h}$ maps poorly to the trend of the data. It is usually caused by a function that is too simple or uses too few features. On the other hand, `overfitting`, or `high variance`, is caused by a hypothesis function that fits the available data but does not generalize well to predict new data. It is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data.\n",
    "\n",
    "To overcome the challenge of overfitting, we may decide to reduce the number of features or by the concept of `regularization` by adding a second term ${\\frac{\\lambda}{2m}} {\\sum_{j=1}^n} {\\theta_j{^2}}$ to the logistic cost function ${J(\\theta)}$ in equation 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Overview of the problem##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the exercise, we will implement `regularized logistic regression` to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly. Now, imagine you are the product manager of the factory and you have the test results for some microchips on two different tests. From these two tests, you would like to determine whether the microchips should be accepted or rejected. To help you make the decision, you have a dataset of test results on past microchips, from which you can build a logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Loading and  visualizing the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 - Loading sample dataset ###\n",
    "\n",
    "Let us load the sample dataset for this exercise, `ex7data2.txt` into pandas dataframe using the `read_csv() function`. The first two columns are features while the third column is the target class column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd() + '\\data\\ex2data2.txt'\n",
    "df = pd.read_csv(path, header=None, names=['Test_1', 'Test_2', 'Accepted'])\n",
    "data=df.values\n",
    "X_train = data[:,[0,1]]\n",
    "y_train = data[:,[2]].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = df[df['Accepted'].isin([1])] # df with records having only ones (1) in the label column 'Admitted' - POSITIVE\n",
    "neg_df = df[df['Accepted'].isin([0])] # df with records having zeros (0) in the label column 'Admitted' - NEGATIVE\n",
    "Ex1_pos = pos_df['Test_1'] \n",
    "Ex2_pos = pos_df['Test_2']\n",
    "Ex1_neg = neg_df['Test_1']\n",
    "Ex2_neg = neg_df['Test_2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 - Visualizing the training data ###\n",
    "\n",
    "Similar to the previous parts of this exercise, `plotData()` is used to visualize the scatter plot of training data, where the axes are the two test scores, and the positive(y = 1, Admitted) and negative (y = 0, Not Admitted) examples are shown with different markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(x1,x2,y1,y2): \n",
    "    plt.scatter(x1,y1, s=50, c='b', marker='o', label='Admitted')  \n",
    "    plt.scatter(x2,y2, s=50, c='r', marker='x', label='Not Admitted')   \n",
    "    plt.xlabel('Microchip Test 1')  \n",
    "    plt.ylabel('Microchip Test 2')\n",
    "    plt.title('Scatter plot of Trainig data')\n",
    "    plt.axis([-1,1.5,-1,1.5])\n",
    "    plt.grid(True)\n",
    "    plt.legend() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotData(Ex1_pos,Ex1_neg,Ex2_pos,Ex2_neg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows that our dataset `cannot be separated into positive and negative examples by a straight-line` through the plot. Therefore, a straight forward application of logistic regression will not perform well on this dataset since logistic regression will only be able to find a linear decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 - Hyperparameter settings ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 1500;\n",
    "alpha = 1;\n",
    "Lambda= 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Mapping features ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of this non-linearity in the training dataset, one way to fit the data better is to create more features from each data point. In the function below `mapFeature()`, we will map the features into all polynomial terms of ${x_1}$ and ${x_2}$ up to the sixth power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This functions maps the features to polynomial features\n",
    "#It returns a new feature array with more features, comprising of X1, X2, X1.^2, X2.^2, X1*X2, X1*X2.^2, etc..\n",
    "def mapFeature(x1,x2):\n",
    "    degree = 6\n",
    "    out = np.ones((x1.shape[0]))\n",
    "    for i in range(1,degree+1):  \n",
    "        for j in range(0,i+1):\n",
    "            tmp = np.multiply(np.power(x1, i-j),np.power(x2, j))\n",
    "            out = np.vstack((out,tmp))\n",
    "    return out.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this mapping is that the vector of features, ${x_1}$ and ${x_2}$ (the scores on two QA tests) will been transformed into a 28-dimensional vector. A logistic regression classifier trained on this higher-dimension feature vector will  a more complex decision boundary and will appear nonlinear when drawn in our 2-dimensional plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mapFeatures](images/mapfeatures.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** While the feature mapping allows us to build a more expressive classifier, it also more susceptible to `overfitting`. \n",
    "\n",
    "In the next parts of the exercise, we will implement regularized logistic regression to fit the data and so that we can see it for ourselves how regularization can help combat the overfitting problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 - Regularized logistic regression objective & gradient functions ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4.1 - Objective function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will implement code to compute the cost function and gradient for regularized logistic regression. Recall that the cost function in logistic regression is given in equation 6. It can be regularized by adding a regularization term,$\\theta_0$. The second sum ${\\frac{\\lambda}{2m}} {\\sum_{j=1}^n} {\\theta_j{^2}}$ ,at the end:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${J{(\\theta)}} = {\\frac{1}{m}}{\\sum_{i=1}^m}{\\biggl[{-y^{(i)}\\,log\\,{(h_\\theta (x^{(i)})})}  - (1-y)^{(i)}\\,log\\,{(1 - h_\\theta (x^ {(i)})})\\biggl] + {\\frac{\\lambda}{2m}} {\\sum_{j=1}^n} {\\theta_j{^2}}}  \\tag{12}$$\n",
    "\n",
    "**Note** that you should not regularize the parameter $\\theta_0$. The second sum ${\\frac{\\lambda}{2m}} {\\sum_{j=1}^n} {\\theta_j{^2}}$  means to explicitly exclude the bias term, $\\theta_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunctionReg(theta, x, y, lamda):\n",
    "    m = x.shape[0]\n",
    "    h = z(x, theta)\n",
    "    pred = sigmoid(h)\n",
    "    unreg_cost = ((-1/m) * (np.dot(y.T,np.log(pred)) + np.dot((1-y).T,np.log(1-pred))))\n",
    "    reg_term = (lamda/(2*m)) * np.sum(np.dot(theta[1:].T,theta[1:])); # Regularization term\n",
    "    cost = unreg_cost + reg_term; # add the regularization term\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = X_train[:,0]  # The first column of X\n",
    "x2 = X_train[:,1]  # The second column of X\n",
    "mappedX = mapFeature(x1,x2)\n",
    "init_theta = np.random.randn(mappedX.shape[1],1)  # random initialization theta with small values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUNNING THE COST FUNCTION\n",
    "cost = costFunctionReg(init_theta, mappedX, y_train, Lambda) #Calling the cost function for the initial cost\n",
    "print('\\nRESULTS ARE AS FOLLOWS:\\n\\nTheta`s shape is: '\\\n",
    "      + str(theta.shape)\\\n",
    "      + '\\nInitial cost is: %f' % (cost[0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 - Gradient descent  algorithm ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the cost function is a vector where the jth element is defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${\n",
    "\\begin{equation*}\n",
    "\\frac {\\partial J(\\theta)}{\\partial \\theta_j} = \n",
    "\\biggl(\\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}\\biggl)  + \\frac{\\lambda}{m} {\\theta_j},\n",
    "\\quad\\quad \\text{for ${j \\ge 1}$}\n",
    "\\tag{13}\n",
    "\\end{equation*}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with linear regression, we will want to separately update $\\theta_0$ and the rest of the parameters because we do not want to regularize $\\theta_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **gradient descent algorithm** will repeat until convergence {\n",
    "$$\\theta_0 = \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=1}^m \\biggl(h_\\theta(x^{(i)})-y^{(i)}\\biggl)x_0^{(i)}$$\n",
    "$$\\theta_j = \\theta_j - \\alpha {\\biggl[\\biggl(\\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}\\biggl) + {\\frac{\\lambda}{m}} {\\theta_j}\\biggl]}$$\n",
    "} where $x_0^{(i)}$ = **1**, **m** is the size of the training set & $j \\ge 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4.3 - Learning parameters using `minimize()`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in the first part of this exercise, we shall adopt `minimize()` function to optimize the objective function. And the method will use is `BFGS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINITION OF AN OBJECTIVE OPTIMIZATION TECHNIQUE USING SCIPY`s minimize(method=â€™BFGSâ€™)\n",
    "# REF: https://docs.scipy.org/doc/scipy-0.17.0/reference/optimize.minimize-bfgs.html\n",
    "def optimizeCostFunction(func,theta,x,y,lamda,iters):\n",
    "    result = opt.minimize(func, x0=theta, args=(x,y,lamda), method='BFGS',options={'disp': False, 'maxiter': 500})\n",
    "    return result.x, result.fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUNNING minimize(method=â€™BFGSâ€™) FUNCTION\n",
    "theta, cost = optimizeCostFunction(costFunctionReg,init_theta,mappedX,y_train,Lambda,num_iters)\n",
    "print('\\nResults are as follows:\\n\\nTheta found by minimize(method=â€™BFGSâ€™): '\\\n",
    "      + str(theta) + '\\n\\nCost at theta found by minimize(method=â€™BFGSâ€™): %.5f' % (cost));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 - Evaluating model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many performance measures, however, in this exercise we shall just compute the model performance on the training data. Ideally, we needed to divide the dataset into train/val/test. Where we train the model on `train` data, evaluate it on `val` data and test it on `test` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(theta, mappedX)  \n",
    "correct = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, y_train)]  \n",
    "accuracy = (sum(map(int, correct)) % len(correct))  \n",
    "print('\\nTraining Accuracy(UNPARTITIONED DATA) = {0}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 - Plotting the decision boundary##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go ahead to plot the decision boundary, let us get some insights of the decision boundary of the logistic regression model. Recall that the hypothesis equation is given by ${h_\\theta{(x)}} = {\\sigma(z)}$, where ${\\sigma(.)}$ denotes the sigmoid function in equation 2. \n",
    "\n",
    "- Intuitively, for classification, when the hypothesis value ${h_\\theta{(x)}} \\ge 0.5$, the model assigns the instance to class \"1\".\n",
    "- We also know that logistic regression is defined as ${h_\\theta{(x)}} = {\\sigma(z)} = {\\sigma{(\\theta^Tx)}}$\n",
    "- This implies ${\\sigma{(\\theta^Tx)}} \\ge 0.5 $ predicts class 1.\n",
    "- A closer look at the sigmoid function graph, ${\\sigma(z)} \\ge 0.5$ when ${z} \\ge 0$, where  ${z = {\\theta^Tx}}$. \n",
    "- This means ${\\theta^T{x} = {\\theta_0 + \\theta_1{x_1} + \\theta_2{x_2}} \\ge 0}$ predicts class \"1\"  .\n",
    "- The decision boundary lets us see the line that has been learned in order to separate out the y=0 vs y=1 classes, in this example.\n",
    "- This boundary is at $h_{\\theta}(x) = 0.5$ (remember, this is the lowest possible value for predicting that a class is \"1\")\n",
    "- This basically means that the `decision boundary` line is ${\\theta_0 + \\theta_1{x_1} + \\theta_2{x_2}} = 0$\n",
    "- The decision boundary will be a line composed of any ${(x_1,x_2)}$ points that make this equation equal zero.\n",
    "\n",
    "- In order to plot the line along the specific data we have, we arbitrarily decide to use values of ${x_{1}}$ from our data, by choosing the max and min, and then add/subtract a little bit in order to make the line fit nicely. Think about it, you could continue down the line in the above equation an infinite amount in either direction, and it will still be the line dividing the two classes. However, we only have data that lies around a certain area of this line, so we make sure to only plot the line and data in that region (otherwise it would just be a line and some blank space around it).\n",
    "\n",
    "- Solve for ${x_{2}}$ since we're using ${x_{1}}$ values (the max & min values +/- 2 in order to make a nice line)\n",
    "\n",
    "$${x_2} = {\\frac {-(\\theta_0 + \\theta_1 x_1)}{\\theta_2}} \\tag{14}$$\n",
    "\n",
    "- Plugin the two ${x_2}$ values (stored in plot_x) into the above equation to get the two corresponding ${x_3}$ values (and store in the plot_y variable).\n",
    "- Plot a line using these values -> this will be the decision boundary\n",
    "- Plot the rest of our data on the graph as well, and notice that the line should separate the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to implement a decision boundary. To help us visualize the model learned by this classifier, we define a function `plotDecisionBoundary()` which plots the (non-linear) decision boundary that separates the positive and negative examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** In plotDecisionBoundary(), we plot the `non-linear decision boundary` by computing the classifier's predictions on an evenly spaced grid and then and drew a contour plot of where the predictions change from y = 0 to y = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LogNorm\n",
    "def plotDecisionBoundary(theta,Lambda):\n",
    "    xvals = np.linspace(-1, 1.5, 50)\n",
    "    yvals = np.linspace(-1, 1.5, 50)\n",
    "    zvals = np.zeros((len(xvals), len(yvals)));\n",
    "    \n",
    "    for i in range(len(xvals)):\n",
    "        for j in range(len(yvals)):\n",
    "            mf = mapFeature(np.array([xvals[i]]), np.array([yvals[j]]));\n",
    "            zvals[i][j] = z(mf, theta);\n",
    "    zvals = zvals.T\n",
    "    plt.scatter(Ex1_pos,Ex2_pos, s=50, c='b', marker='o', label='Admitted')  \n",
    "    plt.scatter(Ex1_neg,Ex2_neg, s=50, c='r', marker='x', label='Not Admitted')   \n",
    "    plt.grid(True)\n",
    "    cs =plt.contour(xvals,yvals, zvals, [0])\n",
    "    plt.title('Decison Boundary(%s = %d)' % (r'$\\lambda$', Lambda))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plotData(Ex1_pos,Ex1_neg,Ex2_pos,Ex2_neg) \n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plotDecisionBoundary(theta,Lambda)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_num = 1\n",
    "Lambdas = [0,1,50,100]\n",
    "plt.figure(figsize=(len(Lambdas) * 3 + 5, 4))\n",
    "plt.subplots_adjust(wspace=.2,hspace=.2)\n",
    "for idx, value in enumerate(Lambdas):\n",
    "    plt.subplot(1, len(Lambdas), plot_num)\n",
    "    theta, _  = optimizeCostFunction(costFunctionReg,init_theta,mappedX,y_train,value,num_iters)\n",
    "    xvals = np.linspace(-1, 1.5, 50)\n",
    "    yvals = np.linspace(-1, 1.5, 50)\n",
    "    zvals = np.zeros((len(xvals), len(yvals)));\n",
    "    \n",
    "    for i in range(len(xvals)):\n",
    "        for j in range(len(yvals)):\n",
    "            mf = mapFeature(np.array([xvals[i]]), np.array([yvals[j]]));\n",
    "            zvals[i][j] = z(mf, theta);\n",
    "    zvals = zvals.T\n",
    "    plt.scatter(Ex1_pos,Ex2_pos, s=50, c='b', marker='o', label='Admitted')  \n",
    "    plt.scatter(Ex1_neg,Ex2_neg, s=50, c='r', marker='x', label='Not Admitted')   \n",
    "    plt.grid(True)\n",
    "    cs =plt.contour(xvals,yvals, zvals, [0])\n",
    "    plt.title('Decison boundary(%s = %d)' % (r'$\\lambda$',value))\n",
    "    plt.legend()\n",
    "    plot_num += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed from the four plots that:\n",
    "\n",
    "- With ${\\lambda = 0}$, No regularization (Overfitting)\n",
    "- With ${\\lambda = 50}$ and ${\\lambda = 100}$, there seem to be much regularization (Underfitting)\n",
    "- With ${\\lambda = 1}$, Looks to be an 'Okeyish' fitting. :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
