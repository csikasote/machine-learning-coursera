{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Exercise 3: Multi-Class Classification and Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to programming exercise 3, Multi-Class Classification and Neural Networks! In this exercise, we will implement one-vs-all logistic regression and neural networks to recognize hand-written digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instruction:**\n",
    "- To run code, click on a corresponding cell and press `Shift+Enter` keys simultaneously or Click `Cell -> Run` Cells.\n",
    "\n",
    "**Objective:**\n",
    "- To implement one-vs-all classification algorithm to classify data points belonging to more than two discrete outcomes.\n",
    "\n",
    "**You will learn how to:**\n",
    "- implement one-vs-all classification\n",
    "    - objective/cost function\n",
    "    - gradient function\n",
    "    - sigmoid function\n",
    "    - scipy for optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages ##\n",
    "\n",
    "First lets run the cell below to import all the packages that you will need for this exercise.\n",
    "- [NumPy](http://www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [Matplotlib](http://matplotlib.org) is a common library to plot graphs in python.\n",
    "- [Scipy](https://docs.scipy.org/doc/scipy-0.17.0/reference/optimize.html) a python library for optimization functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python â‰¥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Import common libr\n",
    "import numpy as np\n",
    "import scipy.io as sio \n",
    "from scipy import optimize as opt\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. - Multi-class Classification##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we will use logistic regression and neural networks to recognize handwritten digits (from 0 to 9). Automated handwritten digit recognition is widely used today - from recognizing zip codes (postal codes) on mail envelopes to recognizing amounts written on bank checks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Load training data ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given a data set in `./data/ex3data1.mat` that contains 5000 training examples of handwritten digits. The `.mat` format means that that the data has been saved in a native Octave/MATLAB matrix format, instead of a text (ASCII) format like a csv-file. These matrices can be read directly into your program by using the load command. After loading, matrices of the correct dimensions and values will appear in your program's memory. The matrix will already be named, so you do not need to assign names to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTING DATA FILES\n",
    "df_path = 'data/ex3data1.mat'\n",
    "data = sio.loadmat(df_path)\n",
    "X = data['X']\n",
    "y = data['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `np.unique()` to uniquely output discrete values contained in the vector `y`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X has shape:  (5000, 400)\n",
      "Y has shape:  (5000, 1)\n",
      "Unique elements in Y: [ 1  2  3  4  5  6  7  8  9 10]\n"
     ]
    }
   ],
   "source": [
    "# RESULTS CHECK\n",
    "print('X has shape: ', X.shape)\n",
    "print('Y has shape: ', y.shape)\n",
    "print('Unique elements in Y: %s' %(np.unique(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 5000 training examples in the provided dataset, where each training example is a 20 pixel by 20 pixel grayscale image of the digit. Each pixel is represented by a floating point number indicating the grayscale intensity at that location. The 20 by 20 grid of pixels is \"unrolled\" into a 400-dimensional vector. Each of these training examples becomes a single row in our data matrix X. This gives us a 5000 by 400 matrix X where every row is a training example for a handwritten digit image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second part of the training set is a 5000-dimensional vector y that contains labels for the training set. To make things more compatible with Octave/MATLAB indexing, where there is no zero index, we have mapped the digit zero to the value ten. Therefore, a \"0\" digit is labeled as \"10\", while the digits \"1\" to \"9\" are labeled as \"1\" to \"9\" in their natural order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Visualizing the data ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us begin by visualizing the subset of the training dataset. We will randomly selects selects 25 rows from `X` and passes those rows to the `displayData()` function. This function maps each row to a 20 pixel by 20 pixel grayscale image and displays the images together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_indices = np.random.permutation(X.shape[0]);\n",
    "sel = X[rand_indices[:25], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a simple display function that takes as input a randomly selected 25 rows...and displays them digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayData(sel):\n",
    "    fig, ax = plt.subplots(nrows =5, ncols=5, sharex=True, sharey=True,)\n",
    "    ax = ax.flatten()\n",
    "    for i in range(25):\n",
    "        img = sel[i].reshape(20,20,order=\"F\")\n",
    "        ax[i].imshow(img, cmap=matplotlib.cm.binary, interpolation=\"nearest\")\n",
    "        ax[i].axis('on')\n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displayData(sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Vectorizing Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will be using multiple `one-vs-all logistic regression models` to build a `multi-class classifier`. Since there are 10 classes, we will need to train 10 separate logistic regression classifiers. \n",
    "\n",
    "We will reuse our logistic regression code from the last exercise to succed in this exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3.1 - Sigmoid function**\n",
    "\n",
    " The sigmoid function  also refered to as `logistic function` or a  `logit` is defined as:\n",
    " $${\\sigma{(z)}} = \\frac{1}{ 1 + {e^{-z}}} \\tag{1}$$\n",
    " \n",
    " where ${z} = {\\theta^T{x}}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+ np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3.2 - The 'z' in sigmoid function**\n",
    "\n",
    "The '${z}$' in the sigmoid function in `equation 1` ${z} = {\\theta^T{x}}$ has a vectorized form ${z} = {X\\theta}$. Next we implement this vectorized form of '${z}$'!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z(x, theta):\n",
    "    return np.dot(x,theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3.3 - Regularized cost function**\n",
    "\n",
    "- `Logistic regression (unregularized)`, the cost function is given by:\n",
    "\n",
    "$${J(\\theta)} = -\\frac{1}{m}{\\sum_{i=1}^m}{\\biggl[{y^{(i)}\\,log\\,{(h_\\theta (x^{(i)})})}  + (1-y)^{(i)}\\,log\\,{(1 - h_\\theta (x^ {(i)})})\\biggl]}\\tag{2}$$\n",
    "\n",
    "     - Its vectorized form(unregularized) as:\n",
    "\n",
    "$$J{(\\theta)} = \\frac{1}{m}{\\biggl[{-y^T\\,log{(h)} -(1-y)^T\\,log{(1 - h)}}\\biggl]}$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For `regularized logistic regression`, the cost function is defined by adding a second term ${\\frac{\\lambda}{2m}} {\\sum_{j=1}^n} {\\theta_j{^2}}$ as can be seen below:\n",
    "$${J{(\\theta)}} = \\frac{1}{m}{\\sum_{i=1}^m}{\\biggl[{-y^{(i)}\\,log\\,{(h_\\theta (x^{(i)})})}  - (1-y)^{(i)}\\,log\\,{(1 - h_\\theta (x^ {(i)})})\\biggl] + {\\frac{\\lambda}{2m}} {\\sum_{j=1}^n} {\\theta_j{^2}}}  \\tag{3}$$\n",
    "\n",
    "    **Note** that we do not regularize the parameter $\\theta_0$. And $\\lambda$ is a regularization parameter and ${m}$ is the number of training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Its vectorized form (regularized) is given by: \n",
    "\n",
    "$${J{(\\theta)}} = {\\biggl[\\frac{1}{m}{\\biggl({-y^T\\,log{(h)} -(1-y)^T\\,log{(1 - h)}}\\biggl)\\biggl]} + {\\frac{\\lambda}{2m}} {\\sum_{j=1}^n} {\\theta_j{^2}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(theta, x, y, Lambda):\n",
    "    m = x.shape[0]\n",
    "    h = z(x, theta)\n",
    "    p = sigmoid(h)\n",
    "    unreg_cost = ((-1/m) * (np.dot(y.T,np.log(p)) + np.dot((1-y).T,np.log(1-p))))\n",
    "    reg_term = (Lambda/(2*m)) * np.sum(np.dot(theta[1:].T,theta[1:]));\n",
    "    cost = unreg_cost + reg_term; # Add the regularization term\n",
    "    return cost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3.4 - Regularized gradient function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the `unregularized cost function` in `equation 2`, its corresponding gradient (unregularized) is given by:\n",
    "\n",
    "$${\\frac {\\partial J(\\theta)}{\\partial \\theta_j}} = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)})-y^{(i)})\\,x_j^{(i)} \\tag{4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Its vectorized form is:\n",
    "$${\\frac {\\partial J(\\theta)}{\\partial \\theta_j}} = \\frac {1}{m}{X^T(\\sigma{(X \\theta) - \\vec y})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For `regularized cost function` ${J(\\theta)}$ given by equation 3, its correspondig gradient is given by: \n",
    "\n",
    "$${\\frac {\\partial J(\\theta)}{\\partial \\theta_0}} = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)})-y^{(i)})\\,x_j^{(i)}$$\n",
    "           \n",
    "           for j = 0.\n",
    "\n",
    "$$ {\\frac {\\partial J(\\theta)}{\\partial \\theta_j}} = \\biggl(\\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)})-y^{(i)})\\,x_j^{(i)}\\biggl)  + {\\frac{\\lambda}{m}} {\\theta_j} \\tag{5}$$\n",
    "\n",
    "  for ${j \\ge 1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Its vectorized form is:\n",
    "$${\\frac {\\partial J(\\theta)}{\\partial \\theta_j}} = \\biggl(\\frac {1}{m}{X^T(\\sigma{(X \\theta) - \\vec y})} \\biggl)  + {\\frac{\\lambda}{m}} {\\theta_j}$$\n",
    "\n",
    "for ${j \\ge 1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientFunction(theta, x, y, Lambda):\n",
    "    m = x.shape[0];\n",
    "    h = z(x, theta);\n",
    "    p = sigmoid(h);\n",
    "    grad_unreg = (1/m) * np.dot(x.T,(p-y));\n",
    "    temp=theta;\n",
    "    temp[0]=0; #forcing theta(1) to be zero\n",
    "    reg_grad_term= temp * Lambda/m; # Regularisation term for gradient calculation\n",
    "    grad = grad_unreg + reg_grad_term; # Gradient\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3.5 - Evaluating the cost and gradient functions**\n",
    "\n",
    "Lets us use the following definitions of a test case: `theta-t`, `X_t`,`y_t`, `Lambda_t` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_t = np.array([[-2],[-1],[1],[2]])\n",
    "X_t = np.reshape(np.array(range(1,16)), (3,5)).T/10 \n",
    "X_t = np.insert(X_t,0,1,axis=1)\n",
    "y_t = (np.array([[1],[0],[1],[0],[1]])>= 0.5)\n",
    "Lambda_t = 3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Run` the following cell to check the cost compute!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computed gradient is:\n",
      "\n",
      "[[ 0.14656137]\n",
      " [-0.54855841]\n",
      " [ 0.72472227]\n",
      " [ 1.39800296]]\n",
      "\n",
      "\n",
      "Computed cost is: 2.534819\n"
     ]
    }
   ],
   "source": [
    "cost = costFunction(theta_t, X_t, y_t, Lambda_t)\n",
    "grad = gradientFunction(theta_t, X_t, y_t, Lambda_t)\n",
    "print('\\nComputed gradient is:\\n\\n' + str(grad)+'\\n')\n",
    "print('\\nComputed cost is: %.6f' % (cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 - One-vs-All classification ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will implement one-vs-all classification by training multiple regularized logistic regression classifiers, one for each of the K classes in our dataset. In the handwritten digits dataset, K = 10, but your code should work for any value of K.\n",
    "\n",
    "In the function `oneVsAll()` below, we train a classiffier for each class in K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INITIAL PARAMETER SETTINGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda= 0.1 #Lamda hyperparameter\n",
    "num_labels = 10 # Number of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneVsAll(x, y, num_labels, Lambda):\n",
    "    x = np.insert(x,0,1,axis=1)\n",
    "    n = x.shape[1]\n",
    "    all_theta = np.zeros((num_labels, n));\n",
    "    print('\\nInitializing optimization process ...\\n')\n",
    "    for c in range(num_labels):\n",
    "        initial_theta= np.zeros((n, 1));\n",
    "        z = 10 if c == 0 else c\n",
    "        logic_y = np.array([1 if u == z else 0 for u in y])\n",
    "        print(str(c+1) + '. Optimizing for handwritten number %d'% z +' ... DONE')\n",
    "        args=(x,logic_y,Lambda)\n",
    "        x0 = initial_theta\n",
    "        opts = {'maxiter' : None, \n",
    "                'disp' : False, \n",
    "                'gtol' : 1e-5, \n",
    "                'norm' : np.inf,\n",
    "                'eps' : 1.4901161193847656e-08} \n",
    "        result = opt.minimize(costFunction, x0, jac=gradientFunction, args=args, \n",
    "                              method='CG', options=opts)\n",
    "        all_theta[[c],:]= (result.x).T;\n",
    "        \"\"\"\n",
    "        #ALTERNATIVELY YOU CAN USE FMIN_CG\n",
    "        theta_res = opt.fmin_cg(logisticCostFunction, x0, fprime=gradientFunction, args=args, disp=False)\n",
    "        all_theta[[c],:] = theta_res\n",
    "        \"\"\"\n",
    "    print('\\nOPTIMIZATION PROCESS COMPLETED SUCCESSFULLY!!!')\n",
    "    return all_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing optimization process ...\n",
      "\n",
      "1. Optimizing for handwritten number 10 ... DONE\n",
      "2. Optimizing for handwritten number 1 ... DONE\n",
      "3. Optimizing for handwritten number 2 ... DONE\n",
      "4. Optimizing for handwritten number 3 ... DONE\n",
      "5. Optimizing for handwritten number 4 ... DONE\n",
      "6. Optimizing for handwritten number 5 ... DONE\n",
      "7. Optimizing for handwritten number 6 ... DONE\n",
      "8. Optimizing for handwritten number 7 ... DONE\n",
      "9. Optimizing for handwritten number 8 ... DONE\n",
      "10. Optimizing for handwritten number 9 ... DONE\n",
      "\n",
      "OPTIMIZATION PROCESS COMPLETED SUCCESSFULLY!!!\n"
     ]
    }
   ],
   "source": [
    "all_theta = oneVsAll(X, y, num_labels, Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4.1 - One-vs-all Prediction**\n",
    "\n",
    "After training our one-vs-all classifier, we can now use it to predict the digit contained in a given image. For each input, you should compute the \"probability\" that it belongs to each class using the trained logistic regression classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one-vs-all prediction function will pick the class for which the corresponding logistic regression classifier outputs the highest probability and return the class label (1, 2,..., or K) as the prediction for the input example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictOneVsAll(all_theta, x):\n",
    "    x = np.insert(x,0,1,axis=1) #Add ones to the X data matrix   \n",
    "    pred = z(x, all_theta.T);\n",
    "    h = sigmoid(pred);\n",
    "    prediction = np.argmax(h, axis=1)\n",
    "    prediction[prediction == 0] = 10 # Replacing 0s in prediction vector with 10\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique elements in Prediction: [ 1  2  3  4  5  6  7  8  9 10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = predictOneVsAll(all_theta, X)\n",
    "print('\\nUnique elements in Prediction: %s\\n' %(np.unique(prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1.4.2 - Training accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Accuracy = 96.44%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = predictOneVsAll(all_theta, X)\n",
    "correct = [1 if a == b else 0 for (a, b) in zip(prediction, y)]\n",
    "accuracy = (sum(map(int, correct)) / float(len(correct)))\n",
    "print('\\nTraining Accuracy = %.2f' % (accuracy * 100)+'%\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous part of this exercise, we implemented multi-class logistic regression to recognize handwritten digits. However, logistic regression cannot form more complex hypotheses as it is only a linear classifier. In this part of the exercise, we will implement a neural network to recognize handwritten digits using the same training set as before. The neural network will be able to represent complex models that form non-linear hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** \n",
    "- To implement the feedforward propagation algorithm on pretrained weights for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You will learn how to:**\n",
    "- load pretrained weights into a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Model Representation ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neural network is as shown in the model representation below. It has `3 layers` - `an input layer`, a `hidden layer` and an `output layer`. Recall that our inputs are pixel values of digit images. Since the images are of size 20x20, this gives us 400 input layer units (excluding the extra bias unit which always outputs +1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model_representation](images/model_representation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size  = 400;  # 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25;   # 25 hidden units\n",
    "num_labels = 10;          # 10 labels, from 1 to 10 (note that we have mapped \"0\" to label 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the training data will be loaded into the variables X and y. We have been provided with a set of network parameters ($\\theta_1$, $\\theta_2$) already pretrained. These are stored in `./data/ex3weights.mat` and will be\n",
    "loaded into `theta1` and `theta2`. These weights(parameters) have dimensions that are sized for a neural network with `25 units` in the second layer and `10 output units` (corresponding to the 10 digit classes) as defined in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Neural Network Parameters Successfully Loaded ...\n",
      "\n",
      "Theta1:  (25, 401)\n",
      "\n",
      "Theta2:  (10, 26)\n"
     ]
    }
   ],
   "source": [
    "df = 'data/ex3weights.mat'\n",
    "param = sio.loadmat(df)\n",
    "theta1 = param['Theta1']\n",
    "theta2 = param['Theta2']\n",
    "print('\\nNeural Network Parameters Successfully Loaded ...\\n')\n",
    "# RESULTS CHECK\n",
    "print('Theta1: ', theta1.shape)\n",
    "print('\\nTheta2: ', theta2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Feedforward Propagation and Prediction##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement feedforward propagation for the neural network which is represented in the model in section 2.1. This can be implemented by following the steps below:\n",
    "\n",
    "- **Step 1:** Add a column of ones (bias, $x_0^{(1)}$) at index 0 of the input X. Call the resultant, **A1**.\n",
    "\n",
    "        - A1 = np.insert(X,0,1,axis=1);\n",
    "- **Step 2:** Multiply A1 with the transpose of W1, call the resultant, Z2\n",
    "    \n",
    "        - Z2 = np.dot(A1,W1.T);\n",
    " \n",
    "- **Step 3:** Compute activations **A2** by first applying the sigmoid function to Z2 then Add a column of one (bias, $x_0^{(2)}$ at index 0.\n",
    " \n",
    "        - A2=np.insert(sigmoid(Z2),0,1,axis=1)\n",
    "        \n",
    "- **Step 4:** Compute **Z3** by multiplying the **A2** and the transpose of W2.\n",
    "\n",
    "        - Z3= np.dot(A2,W2.T);\n",
    " \n",
    "- **Step 5:**Compute the activations in outer layer by computing the sigmoid of Z3.\n",
    "\n",
    "        - A3=sigmoid(Z3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(Theta1, Theta2, x):   \n",
    "    # Feedforward\n",
    "    a1= np.insert(x,0,1,axis=1)# Adding a column of ones to X\n",
    "    z2= np.dot(a1, Theta1.T);\n",
    "    a2 = np.insert(sigmoid(z2),0,1,axis=1) # Insert ones\n",
    "    z3=np.dot(a2,Theta2.T);\n",
    "    a3=sigmoid(z3);\n",
    "    \n",
    "    return np.argmax(a3, axis=1) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the training accuracy...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Accuracy = 97.52%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = predict(theta1, theta2, X)\n",
    "correct = [1 if a == b else 0 for (a, b) in zip(pred, y)]\n",
    "accuracy = (sum(map(int, correct)) / float(len(correct)))\n",
    "print('\\nTraining Accuracy = %.2f' % (accuracy * 100)+'%\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
